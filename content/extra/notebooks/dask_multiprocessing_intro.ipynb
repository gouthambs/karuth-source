{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Processing with Pandas and Dask\n",
    "[Goutham Balaraman](http://gouthamanbalaraman.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a post on [multiprocessing with pandas](http://gouthamanbalaraman.com/blog/distributed-processing-pandas.html) a little over 2 years back. A lot has changed, and I have started to use [`dask`](https://github.com/dask/dask) and [`distributed`](https://github.com/dask/distributed) for distributed computation using `pandas`. Here I will show how to implement the [multiprocessing with pandas](http://gouthamanbalaraman.com/blog/distributed-processing-pandas.html) blog using `dask`.\n",
    "\n",
    "For this example, I will download and use the [NYC Taxi & Limousine](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml) data. In this example I will use the January 2009 Yellow tripdata file (2GB in size), and run on my laptop. Extending to multiple data files and much larger sizes is possible too.\n",
    "\n",
    "We start by importing `dask.dataframe` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any large CSV (and other format) file can be read using a `pandas` like `read_csv` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(r\"C:\\temp\\yellow_tripdata_2009-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to understand that unlike the `pandas` `read_csv`, the above command does not actually load the data. It does some data inference, and leaves the other aspects for later.\n",
    "\n",
    "Using the `npartitions` attribute, we can see how many partitions the data will be broken in for loading. Viewing the raw `df` object would give you a shell of the dataframe with column and datatypes inferred. The actual data is not loaded yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can infer the columns and datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vendor_name', 'Trip_Pickup_DateTime', 'Trip_Dropoff_DateTime',\n",
       "       'Passenger_Count', 'Trip_Distance', 'Start_Lon', 'Start_Lat',\n",
       "       'Rate_Code', 'store_and_forward', 'End_Lon', 'End_Lat', 'Payment_Type',\n",
       "       'Fare_Amt', 'surcharge', 'mta_tax', 'Tip_Amt', 'Tolls_Amt',\n",
       "       'Total_Amt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vendor_name               object\n",
       "Trip_Pickup_DateTime      object\n",
       "Trip_Dropoff_DateTime     object\n",
       "Passenger_Count            int64\n",
       "Trip_Distance            float64\n",
       "Start_Lon                float64\n",
       "Start_Lat                float64\n",
       "Rate_Code                float64\n",
       "store_and_forward        float64\n",
       "End_Lon                  float64\n",
       "End_Lat                  float64\n",
       "Payment_Type              object\n",
       "Fare_Amt                 float64\n",
       "surcharge                float64\n",
       "mta_tax                  float64\n",
       "Tip_Amt                  float64\n",
       "Tolls_Amt                float64\n",
       "Total_Amt                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the length of the dataset can be done by using the `size` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dd.Scalar<size-ag..., dtype=int32>, dask.dataframe.core.Scalar)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = df.size\n",
    "size, type(size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the `size` does not return a value yet. The computation is actually defferred until we `compute` it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "253663434"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "size.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This computation comes back with 25MM rows. This computation actually took a while. This is because when we `compute` size, we are not only calculating the size of the data, but we are also actually loading the dataset. Now you think that is not very efficient. There are a couple of approaches you can take:\n",
    "- If you have access to a (cluster of )computers with large enough RAM, then you can load and persist the data in memory. The subsequent computations will compute in memory and will be a lot faster. This also allows you to do many computations much like using `pandas` but in a distributed paradigb. \n",
    "- Another approach is to setup a whole bunch of deferred computations, and to compute out of core. Then `dask` will intelligently load data and process all the computations once by figuring out the various dependencies. This is a great approach if you don't have a lot of RAM available.\n",
    "\n",
    "Now the way to load data in memory is by using the `persist` method on the `df` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above `persist` call is non-blocking and you need to wait a bit for the data to load. Once it is loaded, you can compute the size as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "253663434"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.size.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That computed instantly. Now you can scale to much larger data sizes and compute in parallel."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
